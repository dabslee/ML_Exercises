{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 14:47:34,400\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:516: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(pid=34068)\u001b[0m WARNING:tensorflow:From c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[36m(pid=34068)\u001b[0m \n",
      "2024-07-03 14:47:51,804\tINFO trainable.py:161 -- Trainable.setup took 12.934 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-07-03 14:47:51,804\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-07-03 14:47:58,458\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-48-11\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0010104341940446334\n",
      "    StateBufferConnector_ms: 0.0064236196604642\n",
      "    ViewRequirementAgentConnector_ms: 0.15085475011305374\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 22.625\n",
      "  episode_media: {}\n",
      "  episode_return_max: 64.0\n",
      "  episode_return_mean: 22.625\n",
      "  episode_return_min: 9.0\n",
      "  episode_reward_max: 64.0\n",
      "  episode_reward_mean: 22.625\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 176\n",
      "  episodes_timesteps_total: 3982\n",
      "  hist_stats:\n",
      "    episode_lengths: [17, 46, 19, 16, 27, 16, 22, 17, 13, 29, 17, 37, 13, 12, 21,\n",
      "      39, 14, 11, 19, 32, 28, 20, 32, 22, 12, 11, 25, 13, 26, 50, 19, 12, 25, 35,\n",
      "      16, 64, 12, 31, 52, 29, 12, 25, 11, 25, 11, 27, 39, 13, 10, 15, 14, 38, 15,\n",
      "      12, 36, 21, 50, 34, 26, 20, 52, 13, 15, 21, 18, 16, 16, 13, 27, 14, 32, 21,\n",
      "      12, 29, 22, 18, 24, 16, 24, 44, 12, 51, 17, 14, 9, 25, 10, 13, 10, 15, 22, 43,\n",
      "      16, 13, 26, 13, 26, 24, 32, 16, 19, 31, 12, 19, 15, 15, 11, 26, 21, 17, 18,\n",
      "      29, 37, 19, 14, 17, 13, 12, 20, 23, 13, 15, 16, 17, 17, 45, 17, 12, 29, 17,\n",
      "      12, 24, 17, 34, 26, 33, 11, 33, 14, 20, 34, 27, 10, 16, 42, 35, 32, 32, 33,\n",
      "      16, 27, 16, 16, 24, 12, 16, 32, 20, 35, 10, 19, 34, 10, 15, 56, 17, 17, 28,\n",
      "      27, 25, 25, 43, 37, 10, 34, 11]\n",
      "    episode_reward: [17.0, 46.0, 19.0, 16.0, 27.0, 16.0, 22.0, 17.0, 13.0, 29.0, 17.0,\n",
      "      37.0, 13.0, 12.0, 21.0, 39.0, 14.0, 11.0, 19.0, 32.0, 28.0, 20.0, 32.0, 22.0,\n",
      "      12.0, 11.0, 25.0, 13.0, 26.0, 50.0, 19.0, 12.0, 25.0, 35.0, 16.0, 64.0, 12.0,\n",
      "      31.0, 52.0, 29.0, 12.0, 25.0, 11.0, 25.0, 11.0, 27.0, 39.0, 13.0, 10.0, 15.0,\n",
      "      14.0, 38.0, 15.0, 12.0, 36.0, 21.0, 50.0, 34.0, 26.0, 20.0, 52.0, 13.0, 15.0,\n",
      "      21.0, 18.0, 16.0, 16.0, 13.0, 27.0, 14.0, 32.0, 21.0, 12.0, 29.0, 22.0, 18.0,\n",
      "      24.0, 16.0, 24.0, 44.0, 12.0, 51.0, 17.0, 14.0, 9.0, 25.0, 10.0, 13.0, 10.0,\n",
      "      15.0, 22.0, 43.0, 16.0, 13.0, 26.0, 13.0, 26.0, 24.0, 32.0, 16.0, 19.0, 31.0,\n",
      "      12.0, 19.0, 15.0, 15.0, 11.0, 26.0, 21.0, 17.0, 18.0, 29.0, 37.0, 19.0, 14.0,\n",
      "      17.0, 13.0, 12.0, 20.0, 23.0, 13.0, 15.0, 16.0, 17.0, 17.0, 45.0, 17.0, 12.0,\n",
      "      29.0, 17.0, 12.0, 24.0, 17.0, 34.0, 26.0, 33.0, 11.0, 33.0, 14.0, 20.0, 34.0,\n",
      "      27.0, 10.0, 16.0, 42.0, 35.0, 32.0, 32.0, 33.0, 16.0, 27.0, 16.0, 16.0, 24.0,\n",
      "      12.0, 16.0, 32.0, 20.0, 35.0, 10.0, 19.0, 34.0, 10.0, 15.0, 56.0, 17.0, 17.0,\n",
      "      28.0, 27.0, 25.0, 25.0, 43.0, 37.0, 10.0, 34.0, 11.0]\n",
      "  num_episodes: 176\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13259785439306065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05324188991356897\n",
      "    mean_inference_ms: 1.0965077229304128\n",
      "    mean_raw_obs_processing_ms: 0.3567670947520383\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6652272939682007\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.7094369352184315\n",
      "        kl: 0.028792002375800173\n",
      "        policy_loss: -0.04617078013978498\n",
      "        total_loss: 8.978281886090514\n",
      "        vf_explained_var: -0.002074864346493957\n",
      "        vf_loss: 9.01869424594346\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_sampled_lifetime: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_lifetime: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 204.61806177222303\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 204.61806177222303\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 14.285714285714283\n",
      "  ram_util_percent: 84.70000000000002\n",
      "pid: 27488\n",
      "time_since_restore: 19.558095932006836\n",
      "time_this_iter_s: 19.558095932006836\n",
      "time_total_s: 19.558095932006836\n",
      "timers:\n",
      "  learn_throughput: 310.256\n",
      "  learn_time_ms: 12892.569\n",
      "  load_throughput: 1102241.377\n",
      "  load_time_ms: 3.629\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6646.344\n",
      "  synch_weights_time_ms: 6.075\n",
      "  training_iteration_time_ms: 19548.616\n",
      "  training_step_time_ms: 19548.616\n",
      "timestamp: 1720036091\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 8000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-48-29\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0031566619873046875\n",
      "    StateBufferConnector_ms: 0.01613616943359375\n",
      "    ViewRequirementAgentConnector_ms: 0.09575009346008301\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.81\n",
      "  episode_media: {}\n",
      "  episode_return_max: 104.0\n",
      "  episode_return_mean: 41.81\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 41.81\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 93\n",
      "  episodes_timesteps_total: 4181\n",
      "  hist_stats:\n",
      "    episode_lengths: [25, 25, 43, 37, 10, 34, 11, 19, 21, 17, 73, 18, 14, 80, 60,\n",
      "      26, 104, 15, 73, 73, 21, 59, 13, 63, 97, 80, 47, 51, 46, 53, 42, 18, 25, 41,\n",
      "      11, 34, 39, 43, 32, 36, 37, 55, 76, 40, 14, 35, 17, 47, 62, 89, 49, 52, 14,\n",
      "      46, 14, 38, 85, 25, 13, 32, 57, 19, 65, 16, 25, 56, 24, 46, 60, 51, 64, 63,\n",
      "      86, 93, 36, 14, 33, 78, 41, 48, 17, 31, 24, 37, 21, 89, 53, 54, 47, 61, 20,\n",
      "      11, 36, 16, 39, 18, 44, 20, 89, 10]\n",
      "    episode_reward: [25.0, 25.0, 43.0, 37.0, 10.0, 34.0, 11.0, 19.0, 21.0, 17.0, 73.0,\n",
      "      18.0, 14.0, 80.0, 60.0, 26.0, 104.0, 15.0, 73.0, 73.0, 21.0, 59.0, 13.0, 63.0,\n",
      "      97.0, 80.0, 47.0, 51.0, 46.0, 53.0, 42.0, 18.0, 25.0, 41.0, 11.0, 34.0, 39.0,\n",
      "      43.0, 32.0, 36.0, 37.0, 55.0, 76.0, 40.0, 14.0, 35.0, 17.0, 47.0, 62.0, 89.0,\n",
      "      49.0, 52.0, 14.0, 46.0, 14.0, 38.0, 85.0, 25.0, 13.0, 32.0, 57.0, 19.0, 65.0,\n",
      "      16.0, 25.0, 56.0, 24.0, 46.0, 60.0, 51.0, 64.0, 63.0, 86.0, 93.0, 36.0, 14.0,\n",
      "      33.0, 78.0, 41.0, 48.0, 17.0, 31.0, 24.0, 37.0, 21.0, 89.0, 53.0, 54.0, 47.0,\n",
      "      61.0, 20.0, 11.0, 36.0, 16.0, 39.0, 18.0, 44.0, 20.0, 89.0, 10.0]\n",
      "  num_episodes: 93\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12808393168905993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05411168924684276\n",
      "    mean_inference_ms: 1.0660710378858607\n",
      "    mean_raw_obs_processing_ms: 0.34497224969379725\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6084612857910895\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.9770251711049388\n",
      "        kl: 0.017353282923930662\n",
      "        policy_loss: -0.034880820052918565\n",
      "        total_loss: 9.014449055989584\n",
      "        vf_explained_var: 0.051381560038494804\n",
      "        vf_loss: 9.044123882888465\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_sampled_lifetime: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_lifetime: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 218.9500620320071\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 218.9500620320071\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 15.761538461538462\n",
      "  ram_util_percent: 84.89615384615385\n",
      "pid: 27488\n",
      "time_since_restore: 37.837629318237305\n",
      "time_this_iter_s: 18.27953338623047\n",
      "time_total_s: 37.837629318237305\n",
      "timers:\n",
      "  learn_throughput: 320.748\n",
      "  learn_time_ms: 12470.856\n",
      "  load_throughput: 2204482.754\n",
      "  load_time_ms: 1.814\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6433.103\n",
      "  synch_weights_time_ms: 3.037\n",
      "  training_iteration_time_ms: 18908.811\n",
      "  training_step_time_ms: 18908.811\n",
      "timestamp: 1720036109\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-48-48\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0031566619873046875\n",
      "    StateBufferConnector_ms: 0.009541034698486328\n",
      "    ViewRequirementAgentConnector_ms: 0.09116935729980469\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 64.5\n",
      "  episode_media: {}\n",
      "  episode_return_max: 222.0\n",
      "  episode_return_mean: 64.5\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 222.0\n",
      "  episode_reward_mean: 64.5\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_timesteps_total: 6450\n",
      "  hist_stats:\n",
      "    episode_lengths: [76, 40, 14, 35, 17, 47, 62, 89, 49, 52, 14, 46, 14, 38, 85,\n",
      "      25, 13, 32, 57, 19, 65, 16, 25, 56, 24, 46, 60, 51, 64, 63, 86, 93, 36, 14,\n",
      "      33, 78, 41, 48, 17, 31, 24, 37, 21, 89, 53, 54, 47, 61, 20, 11, 36, 16, 39,\n",
      "      18, 44, 20, 89, 10, 81, 174, 10, 92, 67, 29, 52, 161, 79, 177, 96, 43, 93, 211,\n",
      "      139, 86, 42, 116, 32, 48, 195, 74, 97, 122, 136, 29, 46, 61, 86, 109, 48, 159,\n",
      "      138, 94, 61, 26, 114, 38, 34, 222, 133, 140]\n",
      "    episode_reward: [76.0, 40.0, 14.0, 35.0, 17.0, 47.0, 62.0, 89.0, 49.0, 52.0, 14.0,\n",
      "      46.0, 14.0, 38.0, 85.0, 25.0, 13.0, 32.0, 57.0, 19.0, 65.0, 16.0, 25.0, 56.0,\n",
      "      24.0, 46.0, 60.0, 51.0, 64.0, 63.0, 86.0, 93.0, 36.0, 14.0, 33.0, 78.0, 41.0,\n",
      "      48.0, 17.0, 31.0, 24.0, 37.0, 21.0, 89.0, 53.0, 54.0, 47.0, 61.0, 20.0, 11.0,\n",
      "      36.0, 16.0, 39.0, 18.0, 44.0, 20.0, 89.0, 10.0, 81.0, 174.0, 10.0, 92.0, 67.0,\n",
      "      29.0, 52.0, 161.0, 79.0, 177.0, 96.0, 43.0, 93.0, 211.0, 139.0, 86.0, 42.0,\n",
      "      116.0, 32.0, 48.0, 195.0, 74.0, 97.0, 122.0, 136.0, 29.0, 46.0, 61.0, 86.0,\n",
      "      109.0, 48.0, 159.0, 138.0, 94.0, 61.0, 26.0, 114.0, 38.0, 34.0, 222.0, 133.0,\n",
      "      140.0]\n",
      "  num_episodes: 42\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13023238647757754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05346887403729131\n",
      "    mean_inference_ms: 1.0812225283593242\n",
      "    mean_raw_obs_processing_ms: 0.34735470855732353\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.575924753053214\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.9937516870277543\n",
      "        kl: 0.009346134273934653\n",
      "        policy_loss: -0.02341369508455197\n",
      "        total_loss: 9.3645538801788\n",
      "        vf_explained_var: 0.07579277433374877\n",
      "        vf_loss: 9.385163720448812\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_sampled_lifetime: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_lifetime: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 214.03657435715638\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 214.03657435715638\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 10.676923076923078\n",
      "  ram_util_percent: 85.89230769230768\n",
      "pid: 27488\n",
      "time_since_restore: 56.53370261192322\n",
      "time_this_iter_s: 18.696073293685913\n",
      "time_total_s: 56.53370261192322\n",
      "timers:\n",
      "  learn_throughput: 328.142\n",
      "  learn_time_ms: 12189.838\n",
      "  load_throughput: 1954095.896\n",
      "  load_time_ms: 2.047\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6639.82\n",
      "  synch_weights_time_ms: 3.634\n",
      "  training_iteration_time_ms: 18835.339\n",
      "  training_step_time_ms: 18835.339\n",
      "timestamp: 1720036128\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-49-07\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009355306625366211\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.10609722137451172\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 97.49\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 97.49\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 97.49\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_timesteps_total: 9749\n",
      "  hist_stats:\n",
      "    episode_lengths: [25, 13, 32, 57, 19, 65, 16, 25, 56, 24, 46, 60, 51, 64, 63,\n",
      "      86, 93, 36, 14, 33, 78, 41, 48, 17, 31, 24, 37, 21, 89, 53, 54, 47, 61, 20,\n",
      "      11, 36, 16, 39, 18, 44, 20, 89, 10, 81, 174, 10, 92, 67, 29, 52, 161, 79, 177,\n",
      "      96, 43, 93, 211, 139, 86, 42, 116, 32, 48, 195, 74, 97, 122, 136, 29, 46, 61,\n",
      "      86, 109, 48, 159, 138, 94, 61, 26, 114, 38, 34, 222, 133, 140, 466, 392, 500,\n",
      "      500, 242, 500, 162, 149, 138, 71, 144, 23, 322, 154, 214]\n",
      "    episode_reward: [25.0, 13.0, 32.0, 57.0, 19.0, 65.0, 16.0, 25.0, 56.0, 24.0, 46.0,\n",
      "      60.0, 51.0, 64.0, 63.0, 86.0, 93.0, 36.0, 14.0, 33.0, 78.0, 41.0, 48.0, 17.0,\n",
      "      31.0, 24.0, 37.0, 21.0, 89.0, 53.0, 54.0, 47.0, 61.0, 20.0, 11.0, 36.0, 16.0,\n",
      "      39.0, 18.0, 44.0, 20.0, 89.0, 10.0, 81.0, 174.0, 10.0, 92.0, 67.0, 29.0, 52.0,\n",
      "      161.0, 79.0, 177.0, 96.0, 43.0, 93.0, 211.0, 139.0, 86.0, 42.0, 116.0, 32.0,\n",
      "      48.0, 195.0, 74.0, 97.0, 122.0, 136.0, 29.0, 46.0, 61.0, 86.0, 109.0, 48.0,\n",
      "      159.0, 138.0, 94.0, 61.0, 26.0, 114.0, 38.0, 34.0, 222.0, 133.0, 140.0, 466.0,\n",
      "      392.0, 500.0, 500.0, 242.0, 500.0, 162.0, 149.0, 138.0, 71.0, 144.0, 23.0, 322.0,\n",
      "      154.0, 214.0]\n",
      "  num_episodes: 15\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13097841371803268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05333289059517076\n",
      "    mean_inference_ms: 1.0913074081081562\n",
      "    mean_raw_obs_processing_ms: 0.3469341716083953\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5546340148936035\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.5885942791618647\n",
      "        kl: 0.007095096744959931\n",
      "        policy_loss: -0.023510673476923857\n",
      "        total_loss: 9.754621196049516\n",
      "        vf_explained_var: -0.06534337260389841\n",
      "        vf_loss: 9.776003329984603\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_sampled_lifetime: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_lifetime: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 205.04407052496376\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 205.04407052496376\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 12.581481481481482\n",
      "  ram_util_percent: 85.04814814814816\n",
      "pid: 27488\n",
      "time_since_restore: 76.05000424385071\n",
      "time_this_iter_s: 19.51630163192749\n",
      "time_total_s: 76.05000424385071\n",
      "timers:\n",
      "  learn_throughput: 325.368\n",
      "  learn_time_ms: 12293.771\n",
      "  load_throughput: 1277825.964\n",
      "  load_time_ms: 3.13\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6701.854\n",
      "  synch_weights_time_ms: 4.75\n",
      "  training_iteration_time_ms: 19003.505\n",
      "  training_step_time_ms: 19003.505\n",
      "timestamp: 1720036147\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-49-28\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009355306625366211\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.10889959335327148\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 132.07\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 132.07\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 132.07\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 13207\n",
      "  hist_stats:\n",
      "    episode_lengths: [24, 46, 60, 51, 64, 63, 86, 93, 36, 14, 33, 78, 41, 48, 17,\n",
      "      31, 24, 37, 21, 89, 53, 54, 47, 61, 20, 11, 36, 16, 39, 18, 44, 20, 89, 10,\n",
      "      81, 174, 10, 92, 67, 29, 52, 161, 79, 177, 96, 43, 93, 211, 139, 86, 42, 116,\n",
      "      32, 48, 195, 74, 97, 122, 136, 29, 46, 61, 86, 109, 48, 159, 138, 94, 61, 26,\n",
      "      114, 38, 34, 222, 133, 140, 466, 392, 500, 500, 242, 500, 162, 149, 138, 71,\n",
      "      144, 23, 322, 154, 214, 500, 187, 500, 425, 500, 367, 287, 500, 500]\n",
      "    episode_reward: [24.0, 46.0, 60.0, 51.0, 64.0, 63.0, 86.0, 93.0, 36.0, 14.0, 33.0,\n",
      "      78.0, 41.0, 48.0, 17.0, 31.0, 24.0, 37.0, 21.0, 89.0, 53.0, 54.0, 47.0, 61.0,\n",
      "      20.0, 11.0, 36.0, 16.0, 39.0, 18.0, 44.0, 20.0, 89.0, 10.0, 81.0, 174.0, 10.0,\n",
      "      92.0, 67.0, 29.0, 52.0, 161.0, 79.0, 177.0, 96.0, 43.0, 93.0, 211.0, 139.0,\n",
      "      86.0, 42.0, 116.0, 32.0, 48.0, 195.0, 74.0, 97.0, 122.0, 136.0, 29.0, 46.0,\n",
      "      61.0, 86.0, 109.0, 48.0, 159.0, 138.0, 94.0, 61.0, 26.0, 114.0, 38.0, 34.0,\n",
      "      222.0, 133.0, 140.0, 466.0, 392.0, 500.0, 500.0, 242.0, 500.0, 162.0, 149.0,\n",
      "      138.0, 71.0, 144.0, 23.0, 322.0, 154.0, 214.0, 500.0, 187.0, 500.0, 425.0, 500.0,\n",
      "      367.0, 287.0, 500.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13187538775156438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05338777051810253\n",
      "    mean_inference_ms: 1.100452909379671\n",
      "    mean_raw_obs_processing_ms: 0.3476838123960381\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5325055482246542\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4217441965375216\n",
      "        kl: 0.004059275798983833\n",
      "        policy_loss: -0.023540641127094145\n",
      "        total_loss: 9.867940299741683\n",
      "        vf_explained_var: -0.14739191147588915\n",
      "        vf_loss: 9.89026314725158\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_sampled_lifetime: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_lifetime: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 197.53169679462118\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 197.53169679462118\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 10.748275862068965\n",
      "  ram_util_percent: 84.09310344827587\n",
      "pid: 27488\n",
      "time_since_restore: 96.30453157424927\n",
      "time_this_iter_s: 20.25452733039856\n",
      "time_total_s: 96.30453157424927\n",
      "timers:\n",
      "  learn_throughput: 324.26\n",
      "  learn_time_ms: 12335.786\n",
      "  load_throughput: 1597282.456\n",
      "  load_time_ms: 2.504\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6909.28\n",
      "  synch_weights_time_ms: 3.8\n",
      "  training_iteration_time_ms: 19252.787\n",
      "  training_step_time_ms: 19252.787\n",
      "timestamp: 1720036168\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-49-49\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.008215665817260742\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.11646366119384766\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 165.96\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 165.96\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 165.96\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 16596\n",
      "  hist_stats:\n",
      "    episode_lengths: [36, 14, 33, 78, 41, 48, 17, 31, 24, 37, 21, 89, 53, 54, 47,\n",
      "      61, 20, 11, 36, 16, 39, 18, 44, 20, 89, 10, 81, 174, 10, 92, 67, 29, 52, 161,\n",
      "      79, 177, 96, 43, 93, 211, 139, 86, 42, 116, 32, 48, 195, 74, 97, 122, 136, 29,\n",
      "      46, 61, 86, 109, 48, 159, 138, 94, 61, 26, 114, 38, 34, 222, 133, 140, 466,\n",
      "      392, 500, 500, 242, 500, 162, 149, 138, 71, 144, 23, 322, 154, 214, 500, 187,\n",
      "      500, 425, 500, 367, 287, 500, 500, 500, 500, 500, 449, 500, 500, 427, 500]\n",
      "    episode_reward: [36.0, 14.0, 33.0, 78.0, 41.0, 48.0, 17.0, 31.0, 24.0, 37.0, 21.0,\n",
      "      89.0, 53.0, 54.0, 47.0, 61.0, 20.0, 11.0, 36.0, 16.0, 39.0, 18.0, 44.0, 20.0,\n",
      "      89.0, 10.0, 81.0, 174.0, 10.0, 92.0, 67.0, 29.0, 52.0, 161.0, 79.0, 177.0, 96.0,\n",
      "      43.0, 93.0, 211.0, 139.0, 86.0, 42.0, 116.0, 32.0, 48.0, 195.0, 74.0, 97.0,\n",
      "      122.0, 136.0, 29.0, 46.0, 61.0, 86.0, 109.0, 48.0, 159.0, 138.0, 94.0, 61.0,\n",
      "      26.0, 114.0, 38.0, 34.0, 222.0, 133.0, 140.0, 466.0, 392.0, 500.0, 500.0, 242.0,\n",
      "      500.0, 162.0, 149.0, 138.0, 71.0, 144.0, 23.0, 322.0, 154.0, 214.0, 500.0, 187.0,\n",
      "      500.0, 425.0, 500.0, 367.0, 287.0, 500.0, 500.0, 500.0, 500.0, 500.0, 449.0,\n",
      "      500.0, 500.0, 427.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13310910528330167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05337432146597435\n",
      "    mean_inference_ms: 1.1103950057577696\n",
      "    mean_raw_obs_processing_ms: 0.34907576158194686\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5306610163181059\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.3945749784709625\n",
      "        kl: 0.006705391855526355\n",
      "        policy_loss: -0.02276528631887769\n",
      "        total_loss: 9.891442835202781\n",
      "        vf_explained_var: -0.21580712686302841\n",
      "        vf_loss: 9.913202301148445\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_sampled_lifetime: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_lifetime: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 192.25747836953593\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 192.25747836953593\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.355172413793106\n",
      "  ram_util_percent: 84.38965517241377\n",
      "pid: 27488\n",
      "time_since_restore: 117.12009644508362\n",
      "time_this_iter_s: 20.81556487083435\n",
      "time_total_s: 117.12009644508362\n",
      "timers:\n",
      "  learn_throughput: 321.346\n",
      "  learn_time_ms: 12447.658\n",
      "  load_throughput: 1916738.947\n",
      "  load_time_ms: 2.087\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7056.686\n",
      "  synch_weights_time_ms: 3.446\n",
      "  training_iteration_time_ms: 19511.561\n",
      "  training_step_time_ms: 19511.561\n",
      "timestamp: 1720036189\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 28000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-50-11\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.008215665817260742\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.12588977813720703\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 205.22\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 205.22\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 205.22\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_timesteps_total: 20522\n",
      "  hist_stats:\n",
      "    episode_lengths: [53, 54, 47, 61, 20, 11, 36, 16, 39, 18, 44, 20, 89, 10, 81,\n",
      "      174, 10, 92, 67, 29, 52, 161, 79, 177, 96, 43, 93, 211, 139, 86, 42, 116, 32,\n",
      "      48, 195, 74, 97, 122, 136, 29, 46, 61, 86, 109, 48, 159, 138, 94, 61, 26, 114,\n",
      "      38, 34, 222, 133, 140, 466, 392, 500, 500, 242, 500, 162, 149, 138, 71, 144,\n",
      "      23, 322, 154, 214, 500, 187, 500, 425, 500, 367, 287, 500, 500, 500, 500, 500,\n",
      "      449, 500, 500, 427, 500, 500, 208, 500, 453, 297, 256, 275, 337, 392, 401, 399,\n",
      "      377]\n",
      "    episode_reward: [53.0, 54.0, 47.0, 61.0, 20.0, 11.0, 36.0, 16.0, 39.0, 18.0, 44.0,\n",
      "      20.0, 89.0, 10.0, 81.0, 174.0, 10.0, 92.0, 67.0, 29.0, 52.0, 161.0, 79.0, 177.0,\n",
      "      96.0, 43.0, 93.0, 211.0, 139.0, 86.0, 42.0, 116.0, 32.0, 48.0, 195.0, 74.0,\n",
      "      97.0, 122.0, 136.0, 29.0, 46.0, 61.0, 86.0, 109.0, 48.0, 159.0, 138.0, 94.0,\n",
      "      61.0, 26.0, 114.0, 38.0, 34.0, 222.0, 133.0, 140.0, 466.0, 392.0, 500.0, 500.0,\n",
      "      242.0, 500.0, 162.0, 149.0, 138.0, 71.0, 144.0, 23.0, 322.0, 154.0, 214.0, 500.0,\n",
      "      187.0, 500.0, 425.0, 500.0, 367.0, 287.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      449.0, 500.0, 500.0, 427.0, 500.0, 500.0, 208.0, 500.0, 453.0, 297.0, 256.0,\n",
      "      275.0, 337.0, 392.0, 401.0, 399.0, 377.0]\n",
      "  num_episodes: 12\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1347675350712388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05346149227991186\n",
      "    mean_inference_ms: 1.1284954446114532\n",
      "    mean_raw_obs_processing_ms: 0.35130418793415485\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5374286126705908\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.6010704280227743\n",
      "        kl: 0.005370294847256402\n",
      "        policy_loss: -0.022132446865240733\n",
      "        total_loss: 9.838372632508637\n",
      "        vf_explained_var: -0.04450701212370267\n",
      "        vf_loss: 9.85969951178438\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_sampled_lifetime: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_lifetime: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 181.85649646572406\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 181.85649646572406\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 12.61290322580645\n",
      "  ram_util_percent: 85.84193548387098\n",
      "pid: 27488\n",
      "time_since_restore: 139.1212785243988\n",
      "time_this_iter_s: 22.001182079315186\n",
      "time_total_s: 139.1212785243988\n",
      "timers:\n",
      "  learn_throughput: 314.998\n",
      "  learn_time_ms: 12698.481\n",
      "  load_throughput: 2236195.438\n",
      "  load_time_ms: 1.789\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7161.364\n",
      "  synch_weights_time_ms: 3.313\n",
      "  training_iteration_time_ms: 19866.39\n",
      "  training_step_time_ms: 19866.39\n",
      "timestamp: 1720036211\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-50-31\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0061986446380615234\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.13279414176940918\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 240.3\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 240.3\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 240.3\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 24030\n",
      "  hist_stats:\n",
      "    episode_lengths: [18, 44, 20, 89, 10, 81, 174, 10, 92, 67, 29, 52, 161, 79, 177,\n",
      "      96, 43, 93, 211, 139, 86, 42, 116, 32, 48, 195, 74, 97, 122, 136, 29, 46, 61,\n",
      "      86, 109, 48, 159, 138, 94, 61, 26, 114, 38, 34, 222, 133, 140, 466, 392, 500,\n",
      "      500, 242, 500, 162, 149, 138, 71, 144, 23, 322, 154, 214, 500, 187, 500, 425,\n",
      "      500, 367, 287, 500, 500, 500, 500, 500, 449, 500, 500, 427, 500, 500, 208, 500,\n",
      "      453, 297, 256, 275, 337, 392, 401, 399, 377, 371, 446, 500, 500, 500, 500, 28,\n",
      "      500, 500]\n",
      "    episode_reward: [18.0, 44.0, 20.0, 89.0, 10.0, 81.0, 174.0, 10.0, 92.0, 67.0,\n",
      "      29.0, 52.0, 161.0, 79.0, 177.0, 96.0, 43.0, 93.0, 211.0, 139.0, 86.0, 42.0,\n",
      "      116.0, 32.0, 48.0, 195.0, 74.0, 97.0, 122.0, 136.0, 29.0, 46.0, 61.0, 86.0,\n",
      "      109.0, 48.0, 159.0, 138.0, 94.0, 61.0, 26.0, 114.0, 38.0, 34.0, 222.0, 133.0,\n",
      "      140.0, 466.0, 392.0, 500.0, 500.0, 242.0, 500.0, 162.0, 149.0, 138.0, 71.0,\n",
      "      144.0, 23.0, 322.0, 154.0, 214.0, 500.0, 187.0, 500.0, 425.0, 500.0, 367.0,\n",
      "      287.0, 500.0, 500.0, 500.0, 500.0, 500.0, 449.0, 500.0, 500.0, 427.0, 500.0,\n",
      "      500.0, 208.0, 500.0, 453.0, 297.0, 256.0, 275.0, 337.0, 392.0, 401.0, 399.0,\n",
      "      377.0, 371.0, 446.0, 500.0, 500.0, 500.0, 500.0, 28.0, 500.0, 500.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13617189005436992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053550807522692395\n",
      "    mean_inference_ms: 1.141309215574549\n",
      "    mean_raw_obs_processing_ms: 0.35288812809057857\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5089876913896171\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4281294606445778\n",
      "        kl: 0.0032388925886525873\n",
      "        policy_loss: -0.0218195520461567\n",
      "        total_loss: 9.872277127542803\n",
      "        vf_explained_var: -0.28698055897989583\n",
      "        vf_loss: 9.89361085789178\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_sampled_lifetime: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_lifetime: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 192.02866614453214\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 192.02866614453214\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 9.96206896551724\n",
      "  ram_util_percent: 85.52413793103449\n",
      "pid: 27488\n",
      "time_since_restore: 159.95874905586243\n",
      "time_this_iter_s: 20.837470531463623\n",
      "time_total_s: 159.95874905586243\n",
      "timers:\n",
      "  learn_throughput: 311.27\n",
      "  learn_time_ms: 12850.591\n",
      "  load_throughput: 2555651.929\n",
      "  load_time_ms: 1.565\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7130.361\n",
      "  synch_weights_time_ms: 3.089\n",
      "  training_iteration_time_ms: 19986.869\n",
      "  training_step_time_ms: 19986.869\n",
      "timestamp: 1720036231\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-50-53\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0061986446380615234\n",
      "    StateBufferConnector_ms: 0.00507354736328125\n",
      "    ViewRequirementAgentConnector_ms: 0.1384143829345703\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 274.45\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 274.45\n",
      "  episode_return_min: 23.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 274.45\n",
      "  episode_reward_min: 23.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 27445\n",
      "  hist_stats:\n",
      "    episode_lengths: [92, 67, 29, 52, 161, 79, 177, 96, 43, 93, 211, 139, 86, 42,\n",
      "      116, 32, 48, 195, 74, 97, 122, 136, 29, 46, 61, 86, 109, 48, 159, 138, 94, 61,\n",
      "      26, 114, 38, 34, 222, 133, 140, 466, 392, 500, 500, 242, 500, 162, 149, 138,\n",
      "      71, 144, 23, 322, 154, 214, 500, 187, 500, 425, 500, 367, 287, 500, 500, 500,\n",
      "      500, 500, 449, 500, 500, 427, 500, 500, 208, 500, 453, 297, 256, 275, 337, 392,\n",
      "      401, 399, 377, 371, 446, 500, 500, 500, 500, 28, 500, 500, 451, 481, 500, 500,\n",
      "      500, 500, 429, 500]\n",
      "    episode_reward: [92.0, 67.0, 29.0, 52.0, 161.0, 79.0, 177.0, 96.0, 43.0, 93.0,\n",
      "      211.0, 139.0, 86.0, 42.0, 116.0, 32.0, 48.0, 195.0, 74.0, 97.0, 122.0, 136.0,\n",
      "      29.0, 46.0, 61.0, 86.0, 109.0, 48.0, 159.0, 138.0, 94.0, 61.0, 26.0, 114.0,\n",
      "      38.0, 34.0, 222.0, 133.0, 140.0, 466.0, 392.0, 500.0, 500.0, 242.0, 500.0, 162.0,\n",
      "      149.0, 138.0, 71.0, 144.0, 23.0, 322.0, 154.0, 214.0, 500.0, 187.0, 500.0, 425.0,\n",
      "      500.0, 367.0, 287.0, 500.0, 500.0, 500.0, 500.0, 500.0, 449.0, 500.0, 500.0,\n",
      "      427.0, 500.0, 500.0, 208.0, 500.0, 453.0, 297.0, 256.0, 275.0, 337.0, 392.0,\n",
      "      401.0, 399.0, 377.0, 371.0, 446.0, 500.0, 500.0, 500.0, 500.0, 28.0, 500.0,\n",
      "      500.0, 451.0, 481.0, 500.0, 500.0, 500.0, 500.0, 429.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13708522525587064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05367220908264457\n",
      "    mean_inference_ms: 1.1503705678356204\n",
      "    mean_raw_obs_processing_ms: 0.35386082814651465\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5333607347742204\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4494109261460522\n",
      "        kl: 0.007841658923595892\n",
      "        policy_loss: -0.023417137591268428\n",
      "        total_loss: 9.879941591652491\n",
      "        vf_explained_var: -0.22274521961007068\n",
      "        vf_loss: 9.902770631031323\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 7905.5\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_sampled_lifetime: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_lifetime: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 182.93148748458125\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 182.93148748458125\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 25.07741935483871\n",
      "  ram_util_percent: 84.84516129032258\n",
      "pid: 27488\n",
      "time_since_restore: 181.8333535194397\n",
      "time_this_iter_s: 21.87460446357727\n",
      "time_total_s: 181.8333535194397\n",
      "timers:\n",
      "  learn_throughput: 304.597\n",
      "  learn_time_ms: 13132.114\n",
      "  load_throughput: 2875108.42\n",
      "  load_time_ms: 1.391\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7057.959\n",
      "  synch_weights_time_ms: 3.087\n",
      "  training_iteration_time_ms: 20195.674\n",
      "  training_step_time_ms: 20195.674\n",
      "timestamp: 1720036253\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-03_14-51-30\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0061986446380615234\n",
      "    StateBufferConnector_ms: 0.004877328872680664\n",
      "    ViewRequirementAgentConnector_ms: 0.13022232055664062\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 306.92\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 306.92\n",
      "  episode_return_min: 23.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 306.92\n",
      "  episode_reward_min: 23.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_timesteps_total: 30692\n",
      "  hist_stats:\n",
      "    episode_lengths: [43, 93, 211, 139, 86, 42, 116, 32, 48, 195, 74, 97, 122, 136,\n",
      "      29, 46, 61, 86, 109, 48, 159, 138, 94, 61, 26, 114, 38, 34, 222, 133, 140, 466,\n",
      "      392, 500, 500, 242, 500, 162, 149, 138, 71, 144, 23, 322, 154, 214, 500, 187,\n",
      "      500, 425, 500, 367, 287, 500, 500, 500, 500, 500, 449, 500, 500, 427, 500, 500,\n",
      "      208, 500, 453, 297, 256, 275, 337, 392, 401, 399, 377, 371, 446, 500, 500, 500,\n",
      "      500, 28, 500, 500, 451, 481, 500, 500, 500, 500, 429, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500]\n",
      "    episode_reward: [43.0, 93.0, 211.0, 139.0, 86.0, 42.0, 116.0, 32.0, 48.0, 195.0,\n",
      "      74.0, 97.0, 122.0, 136.0, 29.0, 46.0, 61.0, 86.0, 109.0, 48.0, 159.0, 138.0,\n",
      "      94.0, 61.0, 26.0, 114.0, 38.0, 34.0, 222.0, 133.0, 140.0, 466.0, 392.0, 500.0,\n",
      "      500.0, 242.0, 500.0, 162.0, 149.0, 138.0, 71.0, 144.0, 23.0, 322.0, 154.0, 214.0,\n",
      "      500.0, 187.0, 500.0, 425.0, 500.0, 367.0, 287.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 449.0, 500.0, 500.0, 427.0, 500.0, 500.0, 208.0, 500.0, 453.0, 297.0,\n",
      "      256.0, 275.0, 337.0, 392.0, 401.0, 399.0, 377.0, 371.0, 446.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 28.0, 500.0, 500.0, 451.0, 481.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      429.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_episodes: 8\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1378081124259686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053769863071802405\n",
      "    mean_inference_ms: 1.1574882471962753\n",
      "    mean_raw_obs_processing_ms: 0.35445147347090794\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5447105588451508\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.3874316307365574\n",
      "        kl: 0.0021152346845236306\n",
      "        policy_loss: -0.023076191959121535\n",
      "        total_loss: 9.873056449685045\n",
      "        vf_explained_var: -0.20573950839299027\n",
      "        vf_loss: 9.895974003371371\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 8835.5\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_sampled_lifetime: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_lifetime: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 109.55583630650258\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 109.55583630650258\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.290000000000006\n",
      "  ram_util_percent: 85.056\n",
      "pid: 27488\n",
      "time_since_restore: 218.35192728042603\n",
      "time_this_iter_s: 36.51857376098633\n",
      "time_total_s: 218.35192728042603\n",
      "timers:\n",
      "  learn_throughput: 271.084\n",
      "  learn_time_ms: 14755.586\n",
      "  load_throughput: 2986491.981\n",
      "  load_time_ms: 1.339\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7066.046\n",
      "  synch_weights_time_ms: 3.23\n",
      "  training_iteration_time_ms: 21827.213\n",
      "  training_step_time_ms: 21827.213\n",
      "timestamp: 1720036290\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={\"env_vars\": {\"PYTHONWARNINGS\": \"ignore::DeprecationWarning\"}})\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "episode_reward = 0\n",
    "terminated = truncated = False\n",
    "\n",
    "obs, info = env.reset()\n",
    "while not terminated and not truncated:\n",
    "    action = algo.compute_single_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved in directory C:\\Users\\brand\\AppData\\Local\\Temp\\tmp0yv4lu5g\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = algo.save().checkpoint.path\n",
    "print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
