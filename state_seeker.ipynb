{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Seeker\n",
    "Consider a robot with state variable $x = (x_0,x_1,...)$ where $x_t \\in \\mathcal{X}$ for every step $t \\in \\mathbb{N}$. At every step $t$, the robot can choose an action $a_t \\in \\mathcal{A}$ according to policy $a_t^\\pi=\\pi(x_t)$ that stochastically sets the robot's state to $x_{t+1}=Δ(x_t,a)$ and earns some reward $r(x_t,x_{t+1},a)$. The robot's goal is to maximize the total reward over an infinite time horizon with discount factor $\\rho$: $\\pi^\\star = \\argmax_{\\pi}{\\sum_{t=0}^{\\infty} {\\rho^t r(x_t, x_{t+1}, a_t^\\pi)}}$.\n",
    "\n",
    "We will use the off-policy TD algorithm (Q-learning) to estimate $Q(x,a)$, the expected reward remaining when taking action $a$ at state $x$; the $\\pi^\\star$ will then simply be to choose the action $a$ that maximizes $Q(x,a)$ at any state $x$. We will achieve this through:\n",
    "1. Let $\\alpha = (\\alpha_0, \\alpha_1, ...)$ be the learning rate with $\\alpha_n \\in (0,1)$, $\\sum_n \\alpha_n = \\infty$, $\\sum_n \\alpha_n^2 < \\infty$\n",
    "2. Randomly generate an initial $Q_0$ matrix and an initial $x_0$. Choose some randomized policy $\\pi$.\n",
    "3. For $n=0,1,2,...$\n",
    "    - Suppose $Q_n$ and $x_n$ are known. Choose action $a_n$ according to $\\pi$ and $x_{n+1} = Δ(x_n,a_n)$.\n",
    "    - Calculate updated values for $Q_{n+1}(x_n,a_n)$ based on the following equation:\n",
    "        $$Q_{n+1}(x_n,a_n) = Q_n(x_n,a_n) + \\alpha_n (r(x_n, x_{n+1}, a_n) + \\rho \\sup_{b \\in \\mathcal{A}} Q_n(x_{n+1},b) - Q_n(x_n,a_n))$$\n",
    "4. Repeat the recursion until $Q_{n+1} \\approx Q_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self, state_space, action_space, discount, next_state, reward):\n",
    "        self.state_space = state_space # \\mathcal{X}  = list of possible states\n",
    "        self.action_space = action_space # \\mathcal{A} = list of possible actions at any state\n",
    "        self.discount = discount # \\rho = the discount factor\n",
    "        self.next_state = next_state # Δ(x_n,a) = the callable that stochastically determines x_{n+1}\n",
    "        self.reward = reward # r(x_n, x_{n+1}, a) = the reward for going from state x_n to x_{n+1} through action a\n",
    "\n",
    "        # other instance variables\n",
    "        self.Q = np.zeros(shape=(state_space.size, action_space.size))\n",
    "\n",
    "    # resets the Q matrix to be all zeros\n",
    "    def reset_Q(self):\n",
    "        self.Q = np.zeros(shape=(self.state_space.size, self.action_space.size))\n",
    "\n",
    "    # runs the Q-learning algorithm\n",
    "    # we use x_0 = 0 and use a uniformly random policy for training\n",
    "    def train_Q(self, learning_rate, max_iterations, tolerance):\n",
    "        x = 0\n",
    "        for step in range(max_iterations):\n",
    "            # determine a and x_{n+1}\n",
    "            a = np.random.choice(self.action_space)\n",
    "            x_next = self.next_state(x, a)\n",
    "\n",
    "            # find Q_{n+1}\n",
    "            Q_update = self.Q[x, a] + learning_rate(step) * (\n",
    "                self.reward(x, x_next, a) +\n",
    "                self.discount * np.max(self.Q[x_next,]) -\n",
    "                self.Q[x,a]\n",
    "            )\n",
    "\n",
    "            # check recursion end condition\n",
    "            if (Q_update - self.Q[x,a] < tolerance):\n",
    "                return\n",
    "\n",
    "            # set Q = Q_{n+1}\n",
    "            self.Q[x, a] = Q_update\n",
    "\n",
    "    # returns the best action to take at state x\n",
    "    def best_action(self, x):\n",
    "        return np.argmax(self.Q[x,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h = 0.7 # probability of remaining high battery upon searching\n",
    "p_l = 0.6 # probability of remaining low battery upon searching\n",
    "# (x, x_next, action) -> probability\n",
    "prob_matrix = np.array([\n",
    "        [\n",
    "            [p_l,    1.0,    0.0], # low to low\n",
    "            [1-p_l,  0.0,    1.0], # low to high\n",
    "        ], [\n",
    "            [1-p_h,  0.0,    0.0], # high to low\n",
    "            [p_h,    1.0,    1.0], # high to high\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "r_s = 1 # reward for searching\n",
    "r_w = 0.1 # reward for waiting\n",
    "r_p = -10 # reward for running out of battery (low -> high)\n",
    "# (x, x_next, action) -> reward\n",
    "reward_matrix = np.array([\n",
    "        [\n",
    "            [r_s,   r_w,     np.nan], # low to low\n",
    "            [r_p,   np.nan,  0], # low to high\n",
    "        ], [\n",
    "            [r_s,   np.nan,  np.nan], # high to low\n",
    "            [r_s,   r_w,     0], # high to high\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "recycler = Robot(\n",
    "    state_space = np.array([0, 1]), # low, high\n",
    "    action_space = np.array([0, 1, 2]), # search, wait, charge\n",
    "    discount = 0.9,\n",
    "    next_state = lambda x,a : 0 if np.random.rand() < prob_matrix[x,:,a][0] else 1,\n",
    "    reward = lambda x,x_next,a : reward_matrix[x,x_next,a],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Robot.train_Q() missing 3 required positional arguments: 'learning_rate', 'max_iterations', and 'tolerance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m recycler\u001b[38;5;241m.\u001b[39mreset_Q()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrecycler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Robot.train_Q() missing 3 required positional arguments: 'learning_rate', 'max_iterations', and 'tolerance'"
     ]
    }
   ],
   "source": [
    "recycler.reset_Q()\n",
    "recycler.train_Q()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
