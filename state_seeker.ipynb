{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Seeker\n",
    "Consider a robot with state variable $x = (x_0,x_1,...)$ where $x_t \\in \\mathcal{X}$ for every step $t \\in \\mathbb{N}$. At every step $t$, the robot can choose an action $a_t \\in \\mathcal{A}$ according to policy $a_t^\\pi=\\pi(x_t)$ that stochastically sets the robot's state to $x_{t+1}=Δ(x_t,a)$ and earns some reward $r(x_t,x_{t+1},a)$. The robot's goal is to maximize the total reward over an infinite time horizon with discount factor $\\rho$: $\\pi^\\star = \\argmax_{\\pi}{\\sum_{t=0}^{\\infty} {\\rho^t r(x_t, x_{t+1}, a_t^\\pi)}}$.\n",
    "\n",
    "We will use the off-policy TD algorithm (Q-learning) to estimate $Q(x,a)$, the expected reward remaining when taking action $a$ at state $x$; the $\\pi^\\star$ will then simply be to choose the action $a$ that maximizes $Q(x,a)$ at any state $x$. We will achieve this through:\n",
    "1. Let $\\alpha = (\\alpha_0, \\alpha_1, ...)$ be the learning rate with $\\alpha_n \\in (0,1)$, $\\sum_n \\alpha_n = \\infty$, $\\sum_n \\alpha_n^2 < \\infty$\n",
    "2. Randomly generate an initial $Q_0$ matrix and an initial $x_0$. Choose some randomized policy $\\pi$.\n",
    "3. For $n=0,1,2,...$\n",
    "    - Suppose $Q_n$ and $x_n$ are known. Choose action $a_n$ according to $\\pi$ and $x_{n+1} = Δ(x_n,a_n)$.\n",
    "    - Calculate updated values for $Q_{n+1}(x_n,a_n)$ based on the following equation:\n",
    "        $$Q_{n+1}(x_n,a_n) = Q_n(x_n,a_n) + \\alpha_n (r(x_n, x_{n+1}, a_n) + \\rho \\sup_{b \\in \\mathcal{A}} Q_n(x_{n+1},b) - Q_n(x_n,a_n))$$\n",
    "4. Repeat the recursion until $Q_{n+1} \\approx Q_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self, state_space, action_space, discount, next_state, reward):\n",
    "        self.state_space = state_space # \\mathcal{X}  = list of possible states\n",
    "        self.action_space = action_space # \\mathcal{A} = list of possible actions at any state\n",
    "        self.discount = discount # \\rho = the discount factor\n",
    "        self.next_state = next_state # Δ(x_n,a) = the callable that stochastically determines x_{n+1}\n",
    "        self.reward = reward # r(x_n, x_{n+1}, a) = the reward for going from state x_n to x_{n+1} through action a\n",
    "\n",
    "        # other instance variables\n",
    "        self.Q = np.zeros(shape=(state_space.size, action_space.size))\n",
    "\n",
    "    # resets the Q matrix to a randomized matrix\n",
    "    # Alternatively, can provide a Q to set the matrix to\n",
    "    # Or can provide a range to randomly select element values from\n",
    "    def reset_Q(self, Q=None, range=[0,1]):\n",
    "        if Q is not None:\n",
    "            self.Q = np.random.rand(self.state_space.size, self.action_space.size) * (range[1] - range[0]) + range[0]\n",
    "\n",
    "    # runs the Q-learning algorithm\n",
    "    # we choose x_0 randomly and use a uniformly random policy for training\n",
    "    def train_Q(self, learning_rate, min_iterations, tolerance, plot=False):\n",
    "        x = np.random.choice(self.state_space)\n",
    "        step = 0\n",
    "        while step < min_iterations or np.abs(Q_change) > tolerance:\n",
    "            # determine a and x_{n+1}\n",
    "            a = np.random.choice(self.action_space)\n",
    "            x_next = self.next_state(x, a)\n",
    "\n",
    "            # find Q_{n+1}\n",
    "            Q_change = learning_rate(step) * (\n",
    "                self.reward(x, x_next, a) +\n",
    "                self.discount * np.max(self.Q[x_next,]) -\n",
    "                self.Q[x,a]\n",
    "            )\n",
    "\n",
    "            # set Q = Q_{n+1} and x = x_next\n",
    "            self.Q[x, a] += Q_change\n",
    "            x = x_next\n",
    "            step += 1\n",
    "        \n",
    "        return self.Q, step\n",
    "\n",
    "    # returns the best action to take at state x\n",
    "    def best_action(self, x):\n",
    "        return np.argmax(self.Q[x,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12.71872979,  5.86021998],\n",
       "        [ 8.60730564, 11.33480161]]),\n",
       " 1000000)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_states = [\"1\", \"2\"]\n",
    "XX = np.arange(len(example_states))\n",
    "example_actions = [\"s\", \"l\"]\n",
    "AA = np.arange(len(example_actions))\n",
    "\n",
    "example_p_matrix = np.array([ # action, x0, x1\n",
    "    [[0.4,0.6],[0.2,0.8]],\n",
    "    [[0.3,0.7],[0.1,0.9]]\n",
    "])\n",
    "example_r_matrix = np.array([ # action, x0, x1\n",
    "    [[14,2],[2,3]],\n",
    "    [[0,0],[2,6]]\n",
    "])\n",
    "\n",
    "example_bot = Robot(\n",
    "    state_space = XX,\n",
    "    action_space = AA,\n",
    "    discount = 0.5,\n",
    "    next_state = lambda x,a : np.random.choice(XX, p=example_p_matrix[a,x,:]),\n",
    "    reward = lambda x,x_next,a : example_r_matrix[a,x,x_next],\n",
    ")\n",
    "\n",
    "example_bot.reset_Q(Q=np.ones(shape=(len(example_states), len(example_actions)))*np.mean(example_r_matrix))\n",
    "example_bot.train_Q(\n",
    "    learning_rate = lambda n : 1/(1+0.1*(n+1)),\n",
    "    min_iterations = 10**6,\n",
    "    tolerance = 10**(-2)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
