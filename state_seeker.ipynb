{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Seeker\n",
    "Consider a robot with state variable $x = (x_0,x_1,...)$ where $x_t \\in \\mathcal{X}$ for every step $t \\in \\mathbb{N}$. At every step $t$, the robot can choose an action $a_t \\in \\mathcal{A}$ according to policy $a_t^\\pi=\\pi(x_t)$ that stochastically sets the robot's state to $x_{t+1}=Δ(x_t,a)$ and earns some reward $r(x_t,x_{t+1},a)$. The robot's goal is to maximize the total reward over an infinite time horizon with discount factor $\\rho$: $\\pi^\\star = \\argmax_{\\pi}{\\sum_{t=0}^{\\infty} {\\rho^t r(x_t, x_{t+1}, a_t^\\pi)}}$.\n",
    "\n",
    "We will use the off-policy TD algorithm (Q-learning) to estimate $Q(x,a)$, the expected reward remaining when taking action $a$ at state $x$; the $\\pi^\\star$ will then simply be to choose the action $a$ that maximizes $Q(x,a)$ at any state $x$. We will achieve this through:\n",
    "1. Let $\\alpha = (\\alpha_0, \\alpha_1, ...)$ be the learning rate with $\\alpha_n \\in (0,1)$, $\\sum_n \\alpha_n = \\infty$, $\\sum_n \\alpha_n^2 < \\infty$\n",
    "2. Randomly generate an initial $Q_0$ matrix and an initial $x_0$. Choose some randomized policy $\\pi$.\n",
    "3. For $n=0,1,2,...$\n",
    "    - Suppose $Q_n$ and $x_n$ are known. Choose action $a_n$ according to $\\pi$ and $x_{n+1} = Δ(x_n,a_n)$.\n",
    "    - Calculate updated values for $Q_{n+1}(x_n,a_n)$ based on the following equation:\n",
    "        $$Q_{n+1}(x_n,a_n) = Q_n(x_n,a_n) + \\alpha_n (r(x_n, x_{n+1}, a_n) + \\rho \\sup_{b \\in \\mathcal{A}} Q_n(x_{n+1},b) - Q_n(x_n,a_n))$$\n",
    "4. Repeat the recursion until $Q_{n+1} \\approx Q_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State_Seeker:\n",
    "    def __init__(self, state_space, action_space, discount, next_state, reward):\n",
    "        self.state_space = state_space # \\mathcal{X}  = list of possible states\n",
    "        self.action_space = action_space # \\mathcal{A} = list of possible actions at any state\n",
    "        self.discount = discount # \\rho = the discount factor\n",
    "        self.next_state = next_state # Δ(x_n,a) = the callable that stochastically determines x_{n+1}\n",
    "        self.reward = reward # r(x_n, x_{n+1}, a) = the reward for going from state x_n to x_{n+1} through action a\n",
    "\n",
    "        # other instance variables\n",
    "        self.Q = np.zeros(shape=(len(state_space), len(action_space)))\n",
    "\n",
    "    # resets the Q matrix to a randomized matrix\n",
    "    # Alternatively, can provide a Q to set the matrix to\n",
    "    # Or can provide a range to randomly select element values from\n",
    "    def reset_Q(self, Q=None, range=[0,1]):\n",
    "        if Q is not None:\n",
    "            self.Q = np.random.rand(len(self.state_space), len(self.action_space)) * (range[1] - range[0]) + range[0]\n",
    "\n",
    "    # runs the Q-learning algorithm\n",
    "    # we choose x_0 randomly and use a uniformly random policy for training\n",
    "    # if plot=True, plots the sum of the squares of the Q matrix elements over the steps\n",
    "    def train_Q(self, learning_rate, min_iterations, tolerance, test_increment=None, test_horizon=None, test_trials=None):\n",
    "\n",
    "        if test_increment != None:\n",
    "            performance = []\n",
    "\n",
    "        x = np.random.choice(self.state_space)\n",
    "        step = 0\n",
    "        while step < min_iterations or np.abs(Q_change) > tolerance:\n",
    "            if test_increment != None and step % test_increment == 0:\n",
    "                performance.append(self.test_performance(test_horizon=test_horizon, trials=test_trials)[0])\n",
    "\n",
    "            # determine a and x_{n+1}\n",
    "            a_index = np.random.randint(0, len(self.action_space))\n",
    "            a = self.action_space[a_index]\n",
    "            x_next = self.next_state(x, a)\n",
    "            x_index = self.state_space.index(x)\n",
    "            x_next_index = self.state_space.index(x_next)\n",
    "\n",
    "            # find Q_{n+1}\n",
    "            Q_change = learning_rate(step) * (\n",
    "                self.reward(x, x_next, a) +\n",
    "                self.discount * np.max(self.Q[x_next_index,]) -\n",
    "                self.Q[x_index,a_index]\n",
    "            )\n",
    "\n",
    "            # set Q = Q_{n+1} and x = x_next\n",
    "            self.Q[x_index, a_index] += Q_change\n",
    "            x = x_next\n",
    "            step += 1\n",
    "            \n",
    "        if test_increment != None:\n",
    "            plt.plot(performance)\n",
    "            plt.xlabel(f\"Q-Learning Step ({test_increment}s)\")\n",
    "            plt.ylabel(\"Estimated Reward\")\n",
    "        return self.Q, step\n",
    "\n",
    "    # returns the value function at state x and the best action to take\n",
    "    def value_action(self, x):\n",
    "        best_index = np.argmax(self.Q[self.state_space.index(x),])\n",
    "        return self.Q[self.state_space.index(x),best_index], self.action_space[best_index]\n",
    "    \n",
    "    # runs a given number of trials of the policy\n",
    "    # return the mean and stdev of the total discounted rewards\n",
    "    def test_performance(self, test_horizon, initial_state=None, trials=1):\n",
    "        results = np.zeros(trials)\n",
    "        for trial in range(trials):\n",
    "            x = initial_state if initial_state != None else np.random.choice(self.state_space)\n",
    "            total_reward = 0\n",
    "            for step in range(test_horizon):\n",
    "                _,a = self.value_action(x)\n",
    "                x_next = self.next_state(x,a)\n",
    "                total_reward += self.discount**step * self.reward(x, x_next, a)\n",
    "            results[trial] = total_reward\n",
    "        return np.mean(results), np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matrix:\n",
      "[[1.58548494 3.52843201 4.90503939]\n",
      " [6.36658426 5.73938151 5.64021668]]\n",
      "\n",
      "Max value of state `low` is 4.905039, achieved by taking action `charge`.\n",
      "Max value of state `high` is 6.366584, achieved by taking action `search`.\n"
     ]
    }
   ],
   "source": [
    "recycler_states = [\"low\", \"high\"]\n",
    "recycler_actions = [\"search\", \"wait\", \"charge\"]\n",
    "\n",
    "def recycler_next_state(x,a):\n",
    "    p_h = 0.8 # probability of remaining high battery upon searching\n",
    "    p_l = 0.7 # probability of remaining low battery upon searching\n",
    "    # (x, x_next, action) -> probability\n",
    "    prob_matrix = np.array([\n",
    "            [\n",
    "                [p_l,    1.0,    0.0], # low to low\n",
    "                [1-p_l,  0.0,    1.0], # low to high\n",
    "            ], [\n",
    "                [1-p_h,  0.0,    0.0], # high to low\n",
    "                [p_h,    1.0,    1.0], # high to high\n",
    "            ],\n",
    "        ])\n",
    "    return np.random.choice(recycler_states, p=prob_matrix[recycler_states.index(x),:,recycler_actions.index(a)])\n",
    "\n",
    "def recycler_reward(x,x_next,a):\n",
    "    r_s = 1 # reward for searching\n",
    "    r_w = 0.1 # reward for waiting\n",
    "    r_p = -10 # reward for running out of battery (low -> high)\n",
    "    # (x, x_next, action) -> reward\n",
    "    reward_matrix = np.array([\n",
    "            [\n",
    "                [r_s,   r_w,     np.nan], # low to low\n",
    "                [r_p,   np.nan,  0], # low to high\n",
    "            ], [\n",
    "                [r_s,   np.nan,  np.nan], # high to low\n",
    "                [r_s,   r_w,     0], # high to high\n",
    "            ],\n",
    "        ])\n",
    "    return reward_matrix[recycler_states.index(x),recycler_states.index(x_next),recycler_actions.index(a)]\n",
    "\n",
    "recycler = State_Seeker(\n",
    "    state_space = recycler_states, # low, high\n",
    "    action_space = recycler_actions, # search, wait, charge\n",
    "    discount = 0.9,\n",
    "    next_state = recycler_next_state,\n",
    "    reward = recycler_reward,\n",
    ")\n",
    "\n",
    "recycler.reset_Q()\n",
    "recycler.train_Q(\n",
    "    learning_rate = lambda n : 1/(1+0.1*n),\n",
    "    min_iterations = 10**5,\n",
    "    tolerance = 10**(-2),\n",
    "    # test_increment=10,\n",
    "    # test_horizon=100,\n",
    "    # test_trials=100,\n",
    ")\n",
    "\n",
    "print(f\"Q matrix:\\n{recycler.Q}\\n\")\n",
    "for state in recycler_states:\n",
    "    value, best_action = recycler.value_action(state)\n",
    "    print(\"Max value of state `%s` is %f, achieved by taking action `%s`.\" % (state, value, best_action))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
